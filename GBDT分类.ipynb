{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##   注意dataframe 类型数据iloc很慢，可以用apply函数尝试，或者替换成ndarray格式\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self,max_depth: int = None,min_samples_split:int = 5,\n",
    "         min_samples_leaf: int = 5,min_impurity_decrease: float =0.0):\n",
    "        '''\n",
    "        min_samples_split:  内部节点再划分所需最小样本数\n",
    "        min_samples_leaf:   叶子节点最少样本数 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝\n",
    "        分裂需要满足的最小增益\n",
    "        max_depth: 最大深度\n",
    "        min_impurity_decrease:分裂需要满足的最小增益\n",
    "        '''\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.nodes = 0  \n",
    "        self.tree = None\n",
    "        self.type_feature = None\n",
    "        self.index = None\n",
    "    def __Gini(self,X,y):\n",
    "        '''\n",
    "        :param data: \n",
    "        :param X: 特征数据\n",
    "        :param y: 目标数据\n",
    "        :return: Gini: 返回该数据每个特征的Gini系数\n",
    "        '''\n",
    "        ##  根据第一个公式\n",
    "        K = np.unique(y)\n",
    "        Gini = 1 - np.sum([(len(X[y == k]) / len(X))**2 for k in K])\n",
    "        \n",
    "        return Gini\n",
    "    \n",
    "    def __typeFeature(self,X):\n",
    "        # 表示特征是否为连续还是离散\n",
    "        n_sample,n_feature = X.shape\n",
    "        self.type_feature = []\n",
    "        ####   特征属性小于10个，认为是离散型数据用0表示，连续性数据用1 表示\n",
    "        for f_idx in range(n_feature):\n",
    "            if len(np.unique(X[:, f_idx]))< 10:\n",
    "                self.type_feature.append(0)\n",
    "            else:\n",
    "                self.type_feature.append(1)\n",
    "        return self.type_feature\n",
    "                \n",
    "\n",
    "    def __binSplitData(self,X,y,index,f_idx,f_val):\n",
    "        ### att 数有数据在第f_idx的特征的所有属性,将不等于 f_val 分为一类，其余分为另一类\n",
    "        ####################    0: 离散类型特征二分方法 1:连续数据   ############################\n",
    "        att=X[:, f_idx]\n",
    "        \n",
    "        if self.type_feature[f_idx]== 0:\n",
    "            X_left = X[att == f_val]\n",
    "            X_right = X[att != f_val]\n",
    "            y_left = y[att == f_val]\n",
    "            y_right = y[att != f_val]\n",
    "            index_left = index[att == f_val]\n",
    "            index_right = index[att != f_val]\n",
    "        else:\n",
    "            X_left = X[att <= f_val]\n",
    "            X_right = X[att >f_val]\n",
    "            y_left = y[att <= f_val]\n",
    "            y_right = y[att > f_val]\n",
    "            index_left = index[att <= f_val]\n",
    "            index_right = index[att > f_val]\n",
    "           ## 切分点和样本点的索引\n",
    "        return X_left, X_right, y_left, y_right,index_left,index_right\n",
    "    \n",
    "    \n",
    "    def __bestSplit(self,X,y,index):\n",
    "        '''\n",
    "           \n",
    "        找到最佳分割特征与特征值\n",
    "        :param X\n",
    "        :return: best_f_idx  最佳分割特征 ， best_f_val 特征值\n",
    "         \n",
    "        '''\n",
    "        best_gain = 0\n",
    "        n_sample,n_feature = X.shape\n",
    "        best_f_idx = None\n",
    "        best_f_val = None\n",
    "        ## 第一个终止条件： 当叶子节点中的样本数小于最小分割值，不再分割\n",
    "        Gini_before= self.__Gini(X,y)\n",
    "        if n_sample < self.min_samples_split:\n",
    "            return best_f_idx,best_f_val       \n",
    "        ##-------------------------通过不断二分的过程 寻找对于某个特征，的最佳分割点---------------------------\n",
    "        for f_idx in range(n_feature):\n",
    "        ##-------------------------如果该特征中的属性个数小于10，则认为是离散数据 type_feature = 0，否则else---------------------------\n",
    "\n",
    "            if self.type_feature[f_idx] == 0:\n",
    "                for f_val in np.unique(X[:, f_idx]):\n",
    "                    ## 当某个特征只有两个类别时，仅仅做一次左右子树的划分，不用重复操作\n",
    "                    if len(np.unique(X[:, f_idx]))== 2 and f_val == np.unique(X[:, f_idx])[0]:\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        X_left, X_right, y_left, y_right,index_left,index_right = self.__binSplitData(X,y,index,f_idx,f_val)\n",
    "                        \n",
    "\n",
    "                    ## 第二个终止条件： 分割后样本数据小于节点的最低样本数，则放弃分割   \n",
    "                        if len(index_left)<self.min_samples_leaf or len(index_right)<self.min_samples_leaf:\n",
    "                            continue\n",
    "                        Gini_after = len(X_left)/len(X) * self.__Gini(X_left,y_left) + len(X_right)/len(X) * self.__Gini(X_right,y_right)\n",
    "                    ## 第三个终止条件，当分裂后的增益小于阈值后者大于目前最大增益\n",
    "                        gain = Gini_before - Gini_after\n",
    "\n",
    "                    ## 第三个终止条件，当分裂后的增益小于阈值后者大于目前最大增益\n",
    "                        if gain < self.min_impurity_decrease or gain < best_gain: \n",
    "                            continue\n",
    "                        else:\n",
    "                            ## 更新最大增益和最佳分裂位置\n",
    "                            best_gain = gain\n",
    "                            best_f_idx,best_f_val = f_idx,f_val\n",
    "        ##-------------------------     连续特征属性的二分 case = 1   ---------------------------\n",
    "            else:\n",
    "                for f_val in np.linspace(X[:, f_idx].min()+1,X[:, f_idx].max()-1,num=50):\n",
    "                        X_left, X_right, y_left, y_right,index_left,index_right = self.__binSplitData(X,y,index,f_idx,f_val)\n",
    "\n",
    "                    ## 第二个终止条件： 分割后样本数据小于节点的最低样本数，则放弃分割   \n",
    "                        if len(index_left)<self.min_samples_leaf or len(index_right)<self.min_samples_leaf:\n",
    "                            continue\n",
    "                        Gini_after = len(X_left)/len(X) * self.__Gini(X_left,y_left) + len(X_right)/len(X) * self.__Gini(X_right,y_right)\n",
    "                    ## 第三个终止条件，当分裂后的增益小于阈值后者大于目前最大增益\n",
    "                        gain = Gini_before - Gini_after\n",
    "\n",
    "                    ## 第三个终止条件，当分裂后的增益小于阈值后者大于目前最大增益\n",
    "                        if gain < self.min_impurity_decrease or gain < best_gain: \n",
    "                            continue\n",
    "                        else:\n",
    "                            ## 更新最大增益和最佳分裂位置\n",
    "                            best_gain = gain\n",
    "                            best_f_idx,best_f_val = f_idx,f_val\n",
    "        return best_f_idx,best_f_val\n",
    "\n",
    "    def __CART(self,X,y,index,probability):\n",
    "        '''\n",
    "        生成CART树\n",
    "        :param X： 特征数据\n",
    "        :param y: 目标数据\n",
    "        :return; CART 树\n",
    "        '''\n",
    "        best_f_idx, best_f_val = self.__bestSplit(X,y,index)\n",
    "        self.nodes += 1\n",
    "        \n",
    "       \n",
    "        # best_f_idx 为空表示不能接续划分，则该点为叶子结点  best_f_val\n",
    "        if best_f_idx is None:\n",
    "            return index\n",
    "        # 节点数超过最大深度的限制，也要返回叶节点，叶节点的值为当前数据中的目标值众数\n",
    "        if self.max_depth:\n",
    "            if self.nodes >= 2**self.max_depth:\n",
    "                return index\n",
    "        tree = dict()\n",
    "        tree['cut_f'] = best_f_idx\n",
    "        tree['cut_val'] = best_f_val\n",
    "        X_left, X_right, y_left, y_right,index_left,index_right = self.__binSplitData(X,y,index,best_f_idx,best_f_val)\n",
    "        tree['left_value'] = np.sum(y_left)/ np.sum(probability[index_left] * (1- probability[index_left]))\n",
    "        tree['right_value'] = np.sum(y_right)/ np.sum(probability[index_right] * (1- probability[index_right]))\n",
    "        tree['left'] = self.__CART(X_left,y_left,index_left,probability)\n",
    "        tree['right'] = self.__CART(X_right,y_right,index_right,probability)\n",
    "        return tree       \n",
    "   \n",
    "    \n",
    "    def fit(self,X,y,probability):\n",
    "        '''\n",
    "        拟合模型，数据应该是 ndarray or series类型，dataframe通过 df.values转变成ndarray，不会报错\n",
    "        :param X: 特征数据\n",
    "        :param: y: 目标数据\n",
    "        :param: sample_weight\n",
    "        :return: None\n",
    "        '''\n",
    "        # 标记每个特征是离散还是连续，从而采用不同的二分方法\n",
    "        self.index = np.array(range(len(X)))\n",
    "        self.type_feature = self.__typeFeature(X) \n",
    "        self.tree = self.__CART(X,y,self.index,probability)\n",
    "        \n",
    "        return self.tree\n",
    "    def predict(self,X_test):\n",
    "        '''\n",
    "        数据类别预测\n",
    "        :param X_test:预测数据\n",
    "        :return: y_: 类别预测结果\n",
    "        '''\n",
    "\n",
    "        return np.array([self.__predict_one(x_test, self.tree) for x_test in X_test])\n",
    "    \n",
    "    def __predict_one(self,x_test,tree,label = None):\n",
    "        if isinstance(tree, dict):  # 非叶节点才做左右判断\n",
    "           \n",
    "            cut_f_idx, cut_val = tree['cut_f'], tree['cut_val']\n",
    "            if self.type_feature[cut_f_idx] == 0:\n",
    "                sub_tree = tree['left'] if x_test[cut_f_idx] == cut_val else tree['right']\n",
    "                label = tree['left_value'] if x_test[cut_f_idx] == cut_val else tree['right_value']\n",
    "            else:\n",
    "                sub_tree = tree['left'] if x_test[cut_f_idx] <= cut_val else tree['right']\n",
    "                label = tree['left_value'] if x_test[cut_f_idx] <=  cut_val else tree['right_value']\n",
    "            return self.__predict_one(x_test, sub_tree,label)\n",
    "        else:\n",
    "            return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTClassifier():\n",
    "    def __init__(self,estimators: int = 10, classifier = DecisionTreeClassifier,step: float = 0.1):\n",
    "        self.estimators = estimators\n",
    "        self.weakLearner = classifier\n",
    "        self.step = step\n",
    "        self.trees = []\n",
    "        self.F_init = None\n",
    "        \n",
    "    def pseudoResiduals(self,y,probability):\n",
    "        rm = y - probability\n",
    "        return rm\n",
    "    \n",
    "    def TerminalRegions(self,tree):\n",
    "        ###  找到每一个叶子节点内的数据，或者说找到叶子节点包含的区域\n",
    "        global Rm\n",
    "        for key, val in tree.items():\n",
    "            if key == 'left' or key =='right':\n",
    "                if isinstance(tree[key],dict):\n",
    "                    self.TerminalRegions(tree[key])\n",
    "                else:\n",
    "                    Rm.append(val)\n",
    "        return Rm\n",
    "    def findRegions(self,x,Rm):\n",
    "        for i in rangr(len(Rm)):\n",
    "            (x == Rm[s[1]]).sum()\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        yes = (y == 1).sum()\n",
    "        no = (y == 0).sum()\n",
    "        self.F_init = np.log(yes/no)\n",
    "        ## step1 通过寻找损失函数最小是对应的来设置初值\n",
    "        F_before = np.array([np.log(yes/no)] * len(X)) \n",
    "        for m in range(self.estimators):\n",
    "            ##(a) 计算损失函数的负梯度值（也叫伪残差）\n",
    "            probability = np.exp(F_before)/(1 + np.exp(F_before))\n",
    "            rm = self.pseudoResiduals(y,probability)\n",
    "            ## (b) 建立学习器拟合以上残差，\n",
    "            tree_clf = self.weakLearner(max_depth = 4)\n",
    "            tree = tree_clf.fit(X, rm,probability)\n",
    "            self.trees.append(tree_clf)\n",
    "            ## 并建立每个叶子节点最终区域\n",
    "            global Rm\n",
    "            Rm = []\n",
    "            Rm = self.TerminalRegions(tree)\n",
    "            Jm = len(Rm)\n",
    "            gamma_m = np.zeros(len(X)) \n",
    "            ## （c）通过最小化每个节点中数据的损失函数和\n",
    "            for j in range(Jm):\n",
    "                gamma_m[Rm[j]] += np.sum(rm[Rm[j]])/ np.sum((probability[Rm[j]] * (1-probability[Rm[j]])))\n",
    "            ##  (d)更新\n",
    "            Fm = F_before + self.step *  gamma_m\n",
    "            F_before = Fm\n",
    "            \n",
    "    def predict(self,x_test):\n",
    "        M = self.estimators\n",
    "        y_ = np.array([self.F_init] * len(x_test)) \n",
    "        for m in range(M):\n",
    "            a=  self.trees[m].predict(X_test)\n",
    "           # print('a.shape -----------------------------',a.shape,y_.shape)\n",
    "            y_ += self.step * self.trees[m].predict(X_test)\n",
    "        probability = np.exp(y_)/(1+np.exp(y_))\n",
    "        return np.int64(probability > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.9122807017543859\n",
      "sklearn acc:0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from sklearn import datasets\n",
    "    import pandas as pd\n",
    "    import  numpy as np    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    data = datasets.load_breast_cancer()\n",
    "    data.target[data.target > 0] = 1\n",
    "    data.target[data.target == 0] = 0\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(data.data)\n",
    "    Y = data.target\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    tree_clf = GBDTClassifier(estimators = 10)\n",
    "    tree_clf.fit(X_train, Y_train)\n",
    "    Y_pred = tree_clf.predict(X_test)\n",
    "    print('acc:{}'.format(np.sum(Y_pred == Y_test) / len(Y_test)))\n",
    "    del tree_clf\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "    tree_clf = GradientBoostingClassifier()\n",
    "\n",
    "    tree_clf.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = tree_clf.predict(X_test)\n",
    "    print('sklearn acc:{}'.format(np.sum(Y_pred == Y_test) / len(Y_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
